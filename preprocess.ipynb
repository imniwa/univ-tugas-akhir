{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "import nltk;import pandas as pd;import nlp_id;import os\n",
    "import string;import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/tsv/data.tsv', sep='\\t')\n",
    "data = df[['content', 'username']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopWord(nlp_id.StopWord):\n",
    "    def __init__(self, stopword_path=None):\n",
    "        self.current_dir = os.path.dirname(os.path.realpath(__name__))\n",
    "        if not stopword_path:\n",
    "            stopword_path = os.path.join(self.current_dir, \"data\", \"pp_stopwords\")\n",
    "        super(StopWord, self).__init__()\n",
    "        with open(stopword_path) as f:\n",
    "            additional = f.read().split('\\n')\n",
    "            self.stopwords = set(self.stopwords).union(set(additional))\n",
    "            \n",
    "        self.stopwords = set(self.stopwords) \\\n",
    "          .union(set(nltk_stopwords.words(\"english\"))) \\\n",
    "          .union(set(nltk_stopwords.words(\"indonesian\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeWord:\n",
    "  def __init__(self, normalize_path=None):\n",
    "    self.current_dir = os.path.dirname(os.path.realpath(__name__))\n",
    "    self.norms_dict = {}\n",
    "    self.multiple_character = {}\n",
    "    if not normalize_path:\n",
    "      normalize_path = os.path.join(self.current_dir, \"data\", \"pp_normalize\")\n",
    "    with open(normalize_path) as f:\n",
    "      fdata = f.read().split(\"\\n\")\n",
    "      for row in fdata:\n",
    "        key, val = tuple(row.split(\":\"))\n",
    "        val = val.split(',')\n",
    "        temp:list = self.norms_dict.get(key, [])\n",
    "        self.norms_dict[key] = list(set([*val, *temp]))\n",
    "        \n",
    "  def normalize(self, content:str):\n",
    "    if re.findall(r'([aiueo]{2,})\\1+', content):\n",
    "      content = re.sub(r'([aiueo]{2,})\\1+', lambda x: x.group(0)[0], content)\n",
    "    for norm, regx in self.norms_dict.items():\n",
    "      if re.findall(rf\"^{'|'.join(regx)}$\", content):\n",
    "        return norm\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = StopWord()\n",
    "tokenizer = nlp_id.Tokenizer()\n",
    "normalizer = NormalizeWord()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(content:str):\n",
    "  text = re.sub(r\"http\\S+\", \"\", content).strip() # remove links\n",
    "  text = re.sub(r\"(@\\w+)\", \"\", text) # remove mentioned\n",
    "  tokens = nltk.tokenize.word_tokenize(text) # tokenizing\n",
    "  tokens = [token.lower() for token in tokens if token not in string.punctuation] # lowercasing + remove punctuation\n",
    "  tokens = [re.sub(r'\\W', \"\", token) for token in tokens if re.findall(r\"\\w+\", token)] # remove unnecessary\n",
    "  tokens = [normalizer.normalize(token) for token in tokens if re.match(r'\\D', token)]\n",
    "  tokens = [x for x in stopwords.remove_stopword(\" \".join(tokens)).split(\" \") if x != '']\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content'] = data['content'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"./data/tsv/cleaned_content.tsv\", index=False, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
