{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "import nltk;import pandas as pd;import nlp_id;import os\n",
    "import string;import re\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/tsv/data.tsv', sep='\\t')\n",
    "data = df[['content', 'username']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopWord(nlp_id.StopWord):\n",
    "    def __init__(self, stopword_path=None):\n",
    "        self.current_dir = os.path.dirname(os.path.realpath(__name__))\n",
    "        if not stopword_path:\n",
    "            stopword_path = os.path.join(self.current_dir, \"data\", \"pp_stopwords\")\n",
    "        super(StopWord, self).__init__()\n",
    "        with open(stopword_path) as f:\n",
    "            additional = f.read().split('\\n')\n",
    "            self.stopwords = set(self.stopwords).union(set(additional))\n",
    "            \n",
    "        self.stopwords = set(self.stopwords) \\\n",
    "          .union(set(nltk_stopwords.words(\"english\"))) \\\n",
    "          .union(set(nltk_stopwords.words(\"indonesian\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeWord:\n",
    "  def __init__(self, normalize_path=None):\n",
    "    self.current_dir = os.path.dirname(os.path.realpath(__name__))\n",
    "    self.norms_dict = {}\n",
    "    self.multiple_character = {}\n",
    "    self.stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "    if not normalize_path:\n",
    "      normalize_path = os.path.join(self.current_dir, \"data\", \"pp_normalize\")\n",
    "    \n",
    "    with open(normalize_path) as f:\n",
    "      fdata = f.read().split(\"\\n\")\n",
    "      for row in fdata:\n",
    "        key, val = tuple(row.split(\":\"))\n",
    "        val = val.split(',')\n",
    "        temp:list = self.norms_dict.get(key, [])\n",
    "        self.norms_dict[key] = list(set([*val, *temp]))\n",
    "        \n",
    "  def normalize(self, content:str):\n",
    "    if re.findall(r'([aiueo]{2,})\\1+', content):\n",
    "      content = re.sub(r'([aiueo]{2,})\\1+', lambda x: x.group(0)[0], content)\n",
    "    \n",
    "    # for norm, regx in self.norms_dict.items():\n",
    "    #   if re.findall(rf\"^{'|'.join(regx)}$\", content):\n",
    "    #     return norm\n",
    "\n",
    "    content = self.stemmer.stem(content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = StopWord()\n",
    "tokenizer = nlp_id.Tokenizer()\n",
    "normalizer = NormalizeWord()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(content:str):\n",
    "  text = re.sub(r\"http\\S+\", \"\", content).strip() # remove links\n",
    "  tokens = nltk.tokenize.word_tokenize(text) # tokenizing\n",
    "  tokens = [token.lower() for token in tokens if token not in string.punctuation] # lowercasing + remove punctuation\n",
    "  tokens = [re.sub(r'\\W', \"\", token) for token in tokens if re.findall(r\"\\w+\", token)] # remove unnecessary\n",
    "  tokens = [x for x in stopwords.remove_stopword(\" \".join(tokens)).split(\" \") if x != '']\n",
    "  tokens = [normalizer.normalize(token) for token in tokens if re.match(r'\\D', token)]\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gak', 'sabar', 'nonton', 'coldplay', 'woootamelon', 'moga', 'dua', 'bagi', 'tiket', 'allah']\n",
      "['idwantscoldplay', 'coldplay', 'bismillah', 'menang', 'war', 'tiket', 'aamiin']\n",
      "['salah', 'tanda', 'coldplay', 'indonesia', 'tur', 'dunia', 'videotron', 'gedung', 'jakarta', 'tanggal', 'mei', 'coldplay', 'konfirmasi', 'datang', 'asia', 'tanggal', 'mei', 'ready', 'coldplayer']\n"
     ]
    }
   ],
   "source": [
    "dest = ['Gak sabar nonton coldplay ðŸ¥¹ðŸ¥² @woootamelon semoga kita berdua kebagian tiketnya ya Allah',\n",
    "'@IDWantsColdplay @coldplay Bismillah menang war tiket AAMIIN',\n",
    "'Salah satu tanda @coldplay ke Indonesia untuk Tur dunia yaitu adanya sebuah Videotron di sejumlah gedung di Jakarta pada tanggal 5-6 Mei ini. Tapi pihak @coldplay akan konfirmasi kedatangan mereka ke Asia tanggal 9 Mei ini. ready Coldplayer https://t.co/zPnaMQ0EZB']\n",
    "for x in dest:\n",
    "  print(preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content'] = data['content'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"./data/tsv/cleaned_content.tsv\", index=False, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
